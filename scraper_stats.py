#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
çˆ¬èŸ²çµ±è¨ˆå·¥å…· - åˆ†ææ­Œæ›²è³‡æ–™åº«
ä½¿ç”¨æ–¹æ³•: python3 scraper_stats.py
"""

import json
import os
from datetime import datetime
from collections import Counter

def analyze_database():
    """åˆ†ææ­Œæ›²è³‡æ–™åº«"""
    
    print("ğŸ¤ å¡æ‹‰OKè³‡æ–™åº«çµ±è¨ˆåˆ†æ")
    print("=" * 40)
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists('public/songs_simplified.json'):
        print("âŒ æ‰¾ä¸åˆ°æ­Œæ›²è³‡æ–™æª”æ¡ˆ")
        return
    
    try:
        with open('public/songs_simplified.json', 'r', encoding='utf-8') as f:
            songs = json.load(f)
        
        print(f"\nğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
        print(f"   ç¸½æ­Œæ›²æ•¸é‡: {len(songs):,} é¦–")
        
        # æŒ‰å…¬å¸çµ±è¨ˆ
        companies = Counter(song.get('å…¬å¸', 'æœªçŸ¥') for song in songs)
        print(f"   æ¶µè“‹å…¬å¸: {len(companies)} å®¶")
        
        # æŒ‰æ­Œæ‰‹çµ±è¨ˆ
        singers = Counter(song.get('æ­Œæ‰‹', 'æœªçŸ¥') for song in songs)
        print(f"   æ”¶éŒ„æ­Œæ‰‹: {len(singers)} ä½")
        
        # æª”æ¡ˆè³‡è¨Š
        file_size = os.path.getsize('public/songs_simplified.json')
        file_size_mb = file_size / (1024 * 1024)
        print(f"   æª”æ¡ˆå¤§å°: {file_size_mb:.2f} MB")
        
        # æœ€å¾Œä¿®æ”¹æ™‚é–“
        mtime = os.path.getmtime('public/songs_simplified.json')
        last_modified = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
        print(f"   æœ€å¾Œæ›´æ–°: {last_modified}")
        
        print(f"\nğŸ¢ å„å…¬å¸æ­Œæ›²æ•¸é‡ (å‰10å):")
        for company, count in companies.most_common(10):
            print(f"   {company}: {count:,} é¦–")
        
        print(f"\nğŸ™ï¸ ç†±é–€æ­Œæ‰‹ (å‰10å):")
        for singer, count in singers.most_common(10):
            print(f"   {singer}: {count} é¦–")
        
        # æª¢æŸ¥é‡è¤‡
        song_titles = [song.get('æ­Œå', '') for song in songs]
        unique_titles = len(set(song_titles))
        duplicate_titles = len(song_titles) - unique_titles
        
        print(f"\nğŸ” è³‡æ–™å“è³ª:")
        print(f"   ç¨ç‰¹æ­Œå: {unique_titles:,}")
        print(f"   é‡è¤‡æ­Œå: {duplicate_titles:,}")
        print(f"   é‡è¤‡ç‡: {duplicate_titles/len(songs)*100:.1f}%")
        
        # æœå°‹å»ºè­°
        print(f"\nğŸ’¡ ç†±é–€æœå°‹å»ºè­°:")
        popular_words = []
        for song in songs[:100]:  # åˆ†æå‰100é¦–æ­Œ
            title = song.get('æ­Œå', '')
            for char in title:
                if '\u4e00' <= char <= '\u9fff':  # ä¸­æ–‡å­—ç¬¦
                    popular_words.append(char)
        
        word_counter = Counter(popular_words)
        print("   å¸¸è¦‹å­—è©: " + " ".join([f"{word}({count})" for word, count in word_counter.most_common(10)]))
        
        # å¢é•·è¶¨å‹¢ï¼ˆå¦‚æœæœ‰æ­·å²è¨˜éŒ„ï¼‰
        if os.path.exists('auto_update.log'):
            print(f"\nğŸ“ˆ å¢é•·è¶¨å‹¢:")
            try:
                with open('auto_update.log', 'r', encoding='utf-8') as f:
                    log_content = f.read()
                    if 'ç›®å‰ç¸½è¨ˆ:' in log_content:
                        import re
                        counts = re.findall(r'ç›®å‰ç¸½è¨ˆ: (\d+) é¦–', log_content)
                        if len(counts) >= 2:
                            start_count = int(counts[0])
                            end_count = int(counts[-1])
                            growth = end_count - start_count
                            print(f"   æœ¬æ¬¡å¢é•·: +{growth:,} é¦–")
                        else:
                            print("   å¢é•·æ•¸æ“šä¸è¶³")
                    else:
                        print("   æ²’æœ‰å¢é•·è¨˜éŒ„")
            except Exception as e:
                print(f"   ç„¡æ³•è®€å–å¢é•·æ•¸æ“š: {e}")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±æ•—: {e}")

def check_scraper_status():
    """æª¢æŸ¥çˆ¬èŸ²é‹è¡Œç‹€æ…‹"""
    import subprocess
    
    print(f"\nğŸ¤– çˆ¬èŸ²ç‹€æ…‹:")
    try:
        # æª¢æŸ¥æ˜¯å¦æœ‰çˆ¬èŸ²ç¨‹åºåœ¨é‹è¡Œ
        result = subprocess.run(['pgrep', '-f', 'auto_update_database.sh|quick_scraper.py'], 
                              capture_output=True, text=True)
        if result.stdout.strip():
            print("   âœ… çˆ¬èŸ²ç¨‹åºæ­£åœ¨é‹è¡Œ")
            pids = result.stdout.strip().split('\n')
            for pid in pids:
                print(f"      PID: {pid}")
        else:
            print("   â¹ï¸  çˆ¬èŸ²ç¨‹åºæœªé‹è¡Œ")
    except Exception as e:
        print(f"   â“ ç„¡æ³•æª¢æŸ¥ç‹€æ…‹: {e}")

if __name__ == "__main__":
    analyze_database()
    check_scraper_status()
    
    print(f"\nğŸŒ ç¶²ç«™: https://karaoke-search-theta.vercel.app")
    print(f"ğŸ“‹ ç›£æ§å‘½ä»¤: ./monitor_scraper.sh")
    print(f"ğŸ”„ é‡æ–°çµ±è¨ˆ: python3 scraper_stats.py")